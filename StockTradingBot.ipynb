{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import os\n",
    "import re\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import warnings\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "warnings.filterwarnings('ignore')\n",
    "            \n",
    "import sys; sys.path.insert(0, '..')\n",
    "from model.models import run_ensemble_strategy, TrainerConfig, Trainer\n",
    "import data.preprocessing as pp\n",
    "from utils.utils_analyze import get_price, check_index_dim\n",
    "from utils.indicators import indicator_list, indicators_stock_stats\n",
    "from env.BaseEnv import EnvConfig\n",
    "from env.EnvStock_val import StockEnvValidation\n",
    "from env.EnvStock_train import StockEnvTrain\n",
    "from env.EnvStock_trade import StockEnvTrade\n",
    "from policy.Policies import MlActorCriticPolicy\n",
    "from config.config import indexes, ticker_list, category_dict, ticker_list_with_dict, tech_tickers\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not Run these two cell if you do not want to preprocess data from the begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2011-01-01'\n",
    "SEED = 42069\n",
    "NORNAMLIZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Run these two cell if you do not want to preprocess data from the begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAC_PARAMS = {'gamma':0.995,\n",
    " 'learning_rate':0.016241092434986143,\n",
    " 'batch_size':64,\n",
    " 'buffer_size':1000000,\n",
    " 'learning_starts':10000,\n",
    " 'gradient_steps':8,\n",
    " 'ent_coef':'auto',\n",
    " 'tau':0.005,\n",
    " 'target_entropy':'auto',\n",
    " 'policy_kwargs':{'log_std_init': -3.3486909298160947,'net_arch': [256, 256] },\n",
    "    'use_sde': False,\n",
    " 'seed':42069,\n",
    "             \"device\":\"cuda\"}\n",
    "\n",
    "\n",
    "PPO_PARAMS =  {'n_steps': 512, 'batch_size': 64, 'gamma': 0.999, 'learning_rate': 0.03691157097485833, \n",
    "                                'ent_coef': 0.08932887565089782, \n",
    "                                'clip_range': 0.1, 'n_epochs': 5, \n",
    "                                'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.891060944163853, \n",
    "                                'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], \n",
    "              'activation_fn': th.nn.ReLU, 'ortho_init': False}, 'seed': 42069, \"device\":\"cuda\"}\n",
    "\n",
    "REC_PARAMS = {\n",
    "    \"net_arch\": [dict(pi=[256, 64, 64], vf=[256, 128])],\n",
    "        \"batch_size\": 32,\n",
    "        \"activation_fn\": nn.modules.activation.Hardswish,\n",
    "        \"learning_rate\":  0.0369,\n",
    "        \"target_step\": 100000,\n",
    "        \"lstm_hidden_size\":256,\n",
    "        \"n_lstm_layers\":2,\n",
    "        \"episode\": 2,\n",
    "        \"seed\":31,\n",
    "        \"shared_lstm\":True\n",
    "}\n",
    "DATASET_VERSION = 'pretrainedTrial6'\n",
    "model = 'reccurent_ppo'\n",
    "MODEL_NAME = f'jimmy_{model}-v1'\n",
    "POPULATION = 10\n",
    "TIME_INTERVAL = 5\n",
    "START_TRADE = '2011-01-01'\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[dict(pi=[256, 128, 64], vf=[256, 128])])\n",
    "TIME_FRAME = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_params = {'n_steps':8,'batch_size':8,'gamma':0.95,'learning_rate':0.027651081986547765,\n",
    " 'ent_coef':0.009004220378275581,'clip_range':0.3,'n_epochs':1,'gae_lambda':0.95,'max_grad_norm':0.9,\n",
    "                 'vf_coef':0.9311987337638572,\n",
    " 'policy_kwargs':{'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': th.nn.Tanh, 'ortho_init': False},\n",
    "                 'seed':42069}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'SAC':SAC_PARAMS, 'PPO':PPO_PARAMS, \"REC_PPO\":REC_PARAMS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconfig = TrainerConfig(start_date=START_TRADE,\n",
    "                        rebalance_window = 126,\n",
    "                        validation_window = 126,\n",
    "                        **{'hparams':winner_params},\n",
    "                        timesteps=50000,\n",
    "                       policy_kwargs = params['REC_PPO'],\n",
    "                       index_list = indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Total number of stocks:   469\n",
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, 'MlpLstmPolicy', \n",
    "                  StockEnvTrain, StockEnvValidation, StockEnvTrade, \n",
    "                  dataset_version=DATASET_VERSION, population=10,\n",
    "                     config = tconfig, model_name= MODEL_NAME, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2016-01-04T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.91240090529124  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2016-01-04 00:00:00 to  2016-07-01 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -4.199516787193716\n",
      "Total Trades:  3343\n",
      "End total asset for validation 1021915.8873350304\n",
      "Mean Reward: 0.5318949534761487\n",
      "STD reward: 0.20680523013041852\n",
      "-----------------\n",
      "Reward for the period is -4.199516787193716\n",
      "Agent #0 has better performance for the training period with total asset: 1021915.8873350304\n",
      "{'batch_size': 128, 'ent_coef': 0.0722302359713579, 'gae_lambda': 0.98, 'max_grad_norm': 0.7, 'vf_coef': 0.2044074893925364, 'gamma': 0.99, 'learning_rate': 0.02013, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.042168887456258  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2016-01-04 00:00:00 to  2016-07-01 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -2.762642385729123\n",
      "Total Trades:  5374\n",
      "End total asset for validation 1048781.2342451978\n",
      "Mean Reward: 1.5790058972779661\n",
      "STD reward: 0.14288359339121007\n",
      "-----------------\n",
      "Reward for the period is -2.762642385729123\n",
      "Agent #1 has better performance for the training period with total asset: 1048781.2342451978\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.357910140355427  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2016-01-04 00:00:00 to  2016-07-01 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -2.762642385729123\n",
      "Total Trades:  5374\n",
      "End total asset for validation 1048781.2342451978\n",
      "Mean Reward: 2.483759423182346\n",
      "STD reward: 0.24244058305438865\n",
      "-----------------\n",
      "Reward for the period is -2.762642385729123\n",
      "{'batch_size': 32, 'ent_coef': 0.05792677794374208, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'vf_coef': 0.9078058089966499, 'gamma': 0.9, 'learning_rate': 0.00465, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.05792677794374208, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'vf_coef': 0.9078058089966499, 'gamma': 0.9, 'learning_rate': 0.00465, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.05792677794374208, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'vf_coef': 0.9078058089966499, 'gamma': 0.9, 'learning_rate': 0.00465, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.09564465828612961, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'vf_coef': 0.8788274877556348, 'gamma': 0.9, 'learning_rate': 0.00481, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.09564465828612961, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'vf_coef': 0.8788274877556348, 'gamma': 0.9, 'learning_rate': 0.00481, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.046910674748384756, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.06436454548009063, 'gamma': 0.999, 'learning_rate': 0.04358, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.046910674748384756, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.06436454548009063, 'gamma': 0.999, 'learning_rate': 0.04358, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2016-07-05T00:00:00.000000000 to  2017-01-03T00:00:00.000000000 Model is :  Rec_PPO\n",
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1000000\n",
      "end_total_asset:1031074.4399413111\n",
      "total_asset_change:31074.43994131114\n",
      "Total cash is: 20541.099941873785$ and total holdings in stocks are 1010533.3399994373$\n",
      "Buy & Hold strategy with previous total asset:  1503890.3334333394\n",
      "Total Cost:  36549.23122215968\n",
      "Sum of rewards  31074.43994131114\n",
      "Total trades:  4849\n",
      "Total buy orders are 2511 and total sell orders are 2338\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  0.08360753479883504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 17:55:15.422 ERROR   wandb.jupyter: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Total reward for the the window is 5.176260463951621\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kaanb (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/2hz8kg0a\" target=\"_blank\">gallant-frog-1121</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24692... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1483401600000000000</td></tr><tr><td>end_total_asset</td><td>1031074.43994</td></tr><tr><td>trade_reward</td><td>-0.14298</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gallant-frog-1121</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/2hz8kg0a\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/2hz8kg0a</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221005_175515-2hz8kg0a\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2016-07-05T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.938544849554697  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2016-07-05 00:00:00 to  2016-12-30 00:00:00\n",
      "-----------------\n",
      "Total Reward:  5.326679207442794\n",
      "Total Trades:  3864\n",
      "End total asset for validation 1070500.7923188857\n",
      "Mean Reward: 3.7390797765692696\n",
      "STD reward: 0.391989511213904\n",
      "-----------------\n",
      "Reward for the period is 5.326679207442794\n",
      "Agent #0 has better performance for the training period with total asset: 1070500.7923188857\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.423440567652385  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2016-07-05 00:00:00 to  2016-12-30 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -0.7936485085665481\n",
      "Total Trades:  5103\n",
      "End total asset for validation 1013132.5881306919\n",
      "Mean Reward: 3.3936763733392583\n",
      "STD reward: 0.23626690622672197\n",
      "-----------------\n",
      "Reward for the period is -0.7936485085665481\n",
      "{'batch_size': 256, 'ent_coef': 0.0073691571633858225, 'gae_lambda': 0.92, 'max_grad_norm': 1.0, 'vf_coef': 0.9595998725524398, 'gamma': 0.99, 'learning_rate': 0.01594, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  8.762379431724549  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2016-07-05 00:00:00 to  2016-12-30 00:00:00\n",
      "-----------------\n",
      "Total Reward:  6.48897179227788\n",
      "Total Trades:  4562\n",
      "End total asset for validation 1044847.5254692452\n",
      "Mean Reward: 3.3245655736129267\n",
      "STD reward: 0.14382273528590075\n",
      "-----------------\n",
      "Reward for the period is 6.48897179227788\n",
      "{'batch_size': 256, 'ent_coef': 0.06621343141839718, 'gae_lambda': 0.95, 'max_grad_norm': 0.7, 'vf_coef': 0.5231531304030377, 'gamma': 0.9, 'learning_rate': 0.04632, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.06621343141839718, 'gae_lambda': 0.95, 'max_grad_norm': 0.7, 'vf_coef': 0.5231531304030377, 'gamma': 0.9, 'learning_rate': 0.04632, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.011004072945972472, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'vf_coef': 0.724727689914811, 'gamma': 0.9999, 'learning_rate': 0.02834, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  8.604233960310617  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2016-07-05 00:00:00 to  2016-12-30 00:00:00\n",
      "-----------------\n",
      "Total Reward:  3.4079658666159958\n",
      "Total Trades:  5329\n",
      "End total asset for validation 1047575.5299334687\n",
      "Mean Reward: 3.8963405534101185\n",
      "STD reward: 0.24545207833113875\n",
      "-----------------\n",
      "Reward for the period is 3.4079658666159958\n",
      "{'batch_size': 256, 'ent_coef': 0.02302052592076046, 'gae_lambda': 0.9, 'max_grad_norm': 0.3, 'vf_coef': 0.5332507324952526, 'gamma': 0.999, 'learning_rate': 0.033, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.02302052592076046, 'gae_lambda': 0.9, 'max_grad_norm': 0.3, 'vf_coef': 0.5332507324952526, 'gamma': 0.999, 'learning_rate': 0.033, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.05810728754302485, 'gae_lambda': 0.98, 'max_grad_norm': 1.0, 'vf_coef': 0.47955245580901384, 'gamma': 0.98, 'learning_rate': 0.0158, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.05810728754302485, 'gae_lambda': 0.98, 'max_grad_norm': 1.0, 'vf_coef': 0.47955245580901384, 'gamma': 0.98, 'learning_rate': 0.0158, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.021453176723643195, 'gae_lambda': 1.0, 'max_grad_norm': 0.7, 'vf_coef': 0.25945128394098005, 'gamma': 0.995, 'learning_rate': 0.03769, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.021453176723643195, 'gae_lambda': 1.0, 'max_grad_norm': 0.7, 'vf_coef': 0.25945128394098005, 'gamma': 0.995, 'learning_rate': 0.03769, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2017-01-03T00:00:00.000000000 to  2017-07-05T00:00:00.000000000 Model is :  Rec_PPO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1031074.4399413111\n",
      "end_total_asset:1140855.8147733626\n",
      "total_asset_change:109781.37483205146\n",
      "Total cash is: 0.07141730242866251$ and total holdings in stocks are 1140855.7433560602$\n",
      "Buy & Hold strategy with previous total asset:  1156671.0253192375\n",
      "Total Cost:  3944.9882072992277\n",
      "Sum of rewards  102040.25338896608\n",
      "Total trades:  746\n",
      "Total buy orders are 397 and total sell orders are 349\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  0.24287091946467665\n",
      "=============\n",
      "Total reward for the the window is 20.87423899251735\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/6hg1p7i4\" target=\"_blank\">mild-breeze-1122</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18140... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1499212800000000000</td></tr><tr><td>end_total_asset</td><td>1140855.81477</td></tr><tr><td>trade_reward</td><td>0.3019</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">mild-breeze-1122</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/6hg1p7i4\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/6hg1p7i4</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221005_183439-6hg1p7i4\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2017-01-03T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.081592718760172  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-01-03 00:00:00 to  2017-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  12.757907731691375\n",
      "Total Trades:  4069\n",
      "End total asset for validation 1124561.241893951\n",
      "Mean Reward: 6.04609273589449\n",
      "STD reward: 0.31962057606989597\n",
      "-----------------\n",
      "Reward for the period is 12.757907731691375\n",
      "Agent #0 has better performance for the training period with total asset: 1124561.241893951\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.593716490268708  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-01-03 00:00:00 to  2017-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  1.6437687575817108\n",
      "Total Trades:  5038\n",
      "End total asset for validation 1008588.7990709435\n",
      "Mean Reward: 5.3805678631179035\n",
      "STD reward: 1.273392932770025\n",
      "-----------------\n",
      "Reward for the period is 1.6437687575817108\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.190284578005473  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-01-03 00:00:00 to  2017-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  12.960870823590085\n",
      "Total Trades:  4067\n",
      "End total asset for validation 1108752.5361250401\n",
      "Mean Reward: 5.676809606549796\n",
      "STD reward: 0.17720225090506211\n",
      "-----------------\n",
      "Reward for the period is 12.960870823590085\n",
      "{'batch_size': 512, 'ent_coef': 0.019312996818552804, 'gae_lambda': 1.0, 'max_grad_norm': 0.3, 'vf_coef': 0.16261023321585188, 'gamma': 0.9999, 'learning_rate': 0.015, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  8.731466766198476  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-01-03 00:00:00 to  2017-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  7.855827779974788\n",
      "Total Trades:  5244\n",
      "End total asset for validation 1042154.508992954\n",
      "Mean Reward: 6.087785097025335\n",
      "STD reward: 1.4054593874398538\n",
      "-----------------\n",
      "Reward for the period is 7.855827779974788\n",
      "{'batch_size': 32, 'ent_coef': 0.052273459744254194, 'gae_lambda': 0.92, 'max_grad_norm': 0.6, 'vf_coef': 0.29246810922423183, 'gamma': 0.98, 'learning_rate': 0.03503, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.052273459744254194, 'gae_lambda': 0.92, 'max_grad_norm': 0.6, 'vf_coef': 0.29246810922423183, 'gamma': 0.98, 'learning_rate': 0.03503, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.043586603624448904, 'gae_lambda': 0.98, 'max_grad_norm': 0.7, 'vf_coef': 0.49358770134638974, 'gamma': 0.995, 'learning_rate': 0.03175, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  9.175966723759968  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-01-03 00:00:00 to  2017-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  9.105191248934716\n",
      "Total Trades:  5255\n",
      "End total asset for validation 1055695.1180511476\n",
      "Mean Reward: 2.217377176193986\n",
      "STD reward: 0.11503259315557698\n",
      "-----------------\n",
      "Reward for the period is 9.105191248934716\n",
      "{'batch_size': 64, 'ent_coef': 0.043586603624448904, 'gae_lambda': 0.98, 'max_grad_norm': 0.7, 'vf_coef': 0.49358770134638974, 'gamma': 0.995, 'learning_rate': 0.03175, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  9.149399971961975  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-01-03 00:00:00 to  2017-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  9.105191248934716\n",
      "Total Trades:  5255\n",
      "End total asset for validation 1055695.1180511476\n",
      "Mean Reward: 5.961473978497088\n",
      "STD reward: 0.4165356640053234\n",
      "-----------------\n",
      "Reward for the period is 9.105191248934716\n",
      "{'batch_size': 32, 'ent_coef': 0.09743977240289654, 'gae_lambda': 0.92, 'max_grad_norm': 0.8, 'vf_coef': 0.14848409801699358, 'gamma': 0.98, 'learning_rate': 0.03107, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.09743977240289654, 'gae_lambda': 0.92, 'max_grad_norm': 0.8, 'vf_coef': 0.14848409801699358, 'gamma': 0.98, 'learning_rate': 0.03107, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 128, 'ent_coef': 0.02551134931858498, 'gae_lambda': 1.0, 'max_grad_norm': 1.0, 'vf_coef': 0.398094420312172, 'gamma': 0.99, 'learning_rate': 0.01021, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 128, 'ent_coef': 0.02551134931858498, 'gae_lambda': 1.0, 'max_grad_norm': 1.0, 'vf_coef': 0.398094420312172, 'gamma': 0.99, 'learning_rate': 0.01021, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 128, 'ent_coef': 0.02551134931858498, 'gae_lambda': 1.0, 'max_grad_norm': 1.0, 'vf_coef': 0.398094420312172, 'gamma': 0.99, 'learning_rate': 0.01021, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2017-07-05T00:00:00.000000000 to  2018-01-03T00:00:00.000000000 Model is :  Rec_PPO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1140855.8147733626\n",
      "end_total_asset:1320673.783592855\n",
      "total_asset_change:179817.9688194925\n",
      "Total cash is: 0.06833382737538951$ and total holdings in stocks are 1320673.7152590277$\n",
      "Buy & Hold strategy with previous total asset:  1289590.2623749394\n",
      "Total Cost:  3699.7725287903622\n",
      "Sum of rewards  179560.93908675946\n",
      "Total trades:  664\n",
      "Total buy orders are 411 and total sell orders are 253\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  0.412301730851117\n",
      "=============\n",
      "Total reward for the the window is 23.226352156314533\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/3hk2znu4\" target=\"_blank\">solar-pine-1123</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22756... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1514937600000000000</td></tr><tr><td>end_total_asset</td><td>1320673.78359</td></tr><tr><td>trade_reward</td><td>0.21985</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">solar-pine-1123</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/3hk2znu4\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/3hk2znu4</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221005_193525-3hk2znu4\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2017-07-05T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.276950486501057  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-07-05 00:00:00 to  2018-01-02 00:00:00\n",
      "-----------------\n",
      "Total Reward:  10.973318701784592\n",
      "Total Trades:  3301\n",
      "End total asset for validation 1101788.8713999193\n",
      "Mean Reward: 8.349754737713374\n",
      "STD reward: 0.07133587739383405\n",
      "-----------------\n",
      "Reward for the period is 10.973318701784592\n",
      "Agent #0 has better performance for the training period with total asset: 1101788.8713999193\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.784900506337484  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-07-05 00:00:00 to  2018-01-02 00:00:00\n",
      "-----------------\n",
      "Total Reward:  3.719995121937245\n",
      "Total Trades:  5018\n",
      "End total asset for validation 1017113.4265737106\n",
      "Mean Reward: 5.811785360384965\n",
      "STD reward: 0.9098271496633322\n",
      "-----------------\n",
      "Reward for the period is 3.719995121937245\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.277083734671274  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-07-05 00:00:00 to  2018-01-02 00:00:00\n",
      "-----------------\n",
      "Total Reward:  11.612709371838719\n",
      "Total Trades:  3424\n",
      "End total asset for validation 1148407.4800339579\n",
      "Mean Reward: 6.933791021857178\n",
      "STD reward: 0.045199258106526245\n",
      "-----------------\n",
      "Reward for the period is 11.612709371838719\n",
      "Agent #2 has better performance for the training period with total asset: 1148407.4800339579\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.372450478871663  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2017-07-05 00:00:00 to  2018-01-02 00:00:00\n",
      "-----------------\n",
      "Total Reward:  13.555827478819992\n",
      "Total Trades:  3329\n",
      "End total asset for validation 1148964.6753607276\n",
      "Mean Reward: 5.879916732339188\n",
      "STD reward: 0.2450324208118213\n",
      "-----------------\n",
      "Reward for the period is 13.555827478819992\n",
      "Agent #3 has better performance for the training period with total asset: 1148964.6753607276\n",
      "{'batch_size': 512, 'ent_coef': 0.012662111686889799, 'gae_lambda': 1.0, 'max_grad_norm': 0.5, 'vf_coef': 0.825235002254999, 'gamma': 0.98, 'learning_rate': 0.0181, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.023886435504436712, 'gae_lambda': 0.98, 'max_grad_norm': 0.9, 'vf_coef': 0.24166642267214855, 'gamma': 0.98, 'learning_rate': 0.03453, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.023886435504436712, 'gae_lambda': 0.98, 'max_grad_norm': 0.9, 'vf_coef': 0.24166642267214855, 'gamma': 0.98, 'learning_rate': 0.03453, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 128, 'ent_coef': 0.07902266696558415, 'gae_lambda': 0.9, 'max_grad_norm': 1.0, 'vf_coef': 0.9635375688492233, 'gamma': 0.9, 'learning_rate': 0.01683, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 128, 'ent_coef': 0.07902266696558415, 'gae_lambda': 0.9, 'max_grad_norm': 1.0, 'vf_coef': 0.9635375688492233, 'gamma': 0.9, 'learning_rate': 0.01683, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 128, 'ent_coef': 0.07902266696558415, 'gae_lambda': 0.9, 'max_grad_norm': 1.0, 'vf_coef': 0.9635375688492233, 'gamma': 0.9, 'learning_rate': 0.01683, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.0927212966158143, 'gae_lambda': 0.92, 'max_grad_norm': 0.9, 'vf_coef': 0.4378520472284748, 'gamma': 0.99, 'learning_rate': 0.04936, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.0927212966158143, 'gae_lambda': 0.92, 'max_grad_norm': 0.9, 'vf_coef': 0.4378520472284748, 'gamma': 0.99, 'learning_rate': 0.04936, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.05834204874235139, 'gae_lambda': 0.8, 'max_grad_norm': 0.8, 'vf_coef': 0.22871764938303196, 'gamma': 0.98, 'learning_rate': 0.03234, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (182, 50)) of distribution Normal(loc: torch.Size([182, 50]), scale: torch.Size([182, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 256, 'ent_coef': 0.056434853773618204, 'gae_lambda': 0.92, 'max_grad_norm': 0.6, 'vf_coef': 0.17052744642968665, 'gamma': 0.995, 'learning_rate': 0.01203, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2018-01-03T00:00:00.000000000 to  2018-07-05T00:00:00.000000000 Model is :  Rec_PPO\n",
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1320673.783592855\n",
      "end_total_asset:1368366.8983591516\n",
      "total_asset_change:47693.114766296465\n",
      "Total cash is: 0.004945497061588533$ and total holdings in stocks are 1368366.8934136545$\n",
      "Buy & Hold strategy with previous total asset:  1355767.6228782823\n",
      "Total Cost:  16243.986547033055\n",
      "Sum of rewards  33636.35053780535\n",
      "Total trades:  1838\n",
      "Total buy orders are 993 and total sell orders are 845\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  0.0609543619130594\n",
      "=============\n",
      "Total reward for the the window is 31.53776096011279\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1yi06n4l\" target=\"_blank\">exalted-shadow-1124</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8788... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1530748800000000000</td></tr><tr><td>end_total_asset</td><td>1368366.89836</td></tr><tr><td>trade_reward</td><td>-0.28976</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-shadow-1124</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1yi06n4l\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/1yi06n4l</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221005_202700-1yi06n4l\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2018-01-03T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.292533377806345  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-01-03 00:00:00 to  2018-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  4.571677113533951\n",
      "Total Trades:  4026\n",
      "End total asset for validation 961652.4474983832\n",
      "Mean Reward: -1.2147369057754986\n",
      "STD reward: 0.036726062113217724\n",
      "-----------------\n",
      "Reward for the period is 4.571677113533951\n",
      "Agent #0 has better performance for the training period with total asset: 961652.4474983832\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.89491709470749  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-01-03 00:00:00 to  2018-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  0.925415676087141\n",
      "Total Trades:  4956\n",
      "End total asset for validation 950771.9883891576\n",
      "Mean Reward: 0.20860039093531668\n",
      "STD reward: 0.11291854787925434\n",
      "-----------------\n",
      "Reward for the period is 0.925415676087141\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.385600451628367  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-01-03 00:00:00 to  2018-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  6.883629592135549\n",
      "Total Trades:  3168\n",
      "End total asset for validation 1055483.1740414426\n",
      "Mean Reward: 1.0861887977458538\n",
      "STD reward: 0.055283717063475464\n",
      "-----------------\n",
      "Reward for the period is 6.883629592135549\n",
      "Agent #2 has better performance for the training period with total asset: 1055483.1740414426\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.34845046599706  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-01-03 00:00:00 to  2018-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  4.907678799703717\n",
      "Total Trades:  3355\n",
      "End total asset for validation 1018285.8468546476\n",
      "Mean Reward: 1.3453745854087173\n",
      "STD reward: 0.04956650710796717\n",
      "-----------------\n",
      "Reward for the period is 4.907678799703717\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.30646622578303  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-01-03 00:00:00 to  2018-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  2.7212198921479285\n",
      "Total Trades:  3320\n",
      "End total asset for validation 988602.5886671102\n",
      "Mean Reward: -0.5572515637148172\n",
      "STD reward: 0.019149693745802436\n",
      "-----------------\n",
      "Reward for the period is 2.7212198921479285\n",
      "{'batch_size': 128, 'ent_coef': 0.04194334672666661, 'gae_lambda': 0.98, 'max_grad_norm': 0.6, 'vf_coef': 0.5414699158941961, 'gamma': 0.9999, 'learning_rate': 0.04062, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  8.826649971803029  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-01-03 00:00:00 to  2018-07-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  2.369916108320467\n",
      "Total Trades:  5146\n",
      "End total asset for validation 949775.4655946902\n",
      "Mean Reward: -0.04651076575682964\n",
      "STD reward: 0.2388113200831365\n",
      "-----------------\n",
      "Reward for the period is 2.369916108320467\n",
      "{'batch_size': 256, 'ent_coef': 0.0025245025405407767, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.27587574702868933, 'gamma': 0.9999, 'learning_rate': 0.03413, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.0025245025405407767, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.27587574702868933, 'gamma': 0.9999, 'learning_rate': 0.03413, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.0272862481654399, 'gae_lambda': 0.8, 'max_grad_norm': 0.3, 'vf_coef': 0.11149679617140429, 'gamma': 0.95, 'learning_rate': 0.02855, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.0272862481654399, 'gae_lambda': 0.8, 'max_grad_norm': 0.3, 'vf_coef': 0.11149679617140429, 'gamma': 0.95, 'learning_rate': 0.02855, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 256, 'ent_coef': 0.0272862481654399, 'gae_lambda': 0.8, 'max_grad_norm': 0.3, 'vf_coef': 0.11149679617140429, 'gamma': 0.95, 'learning_rate': 0.02855, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.08949030960724885, 'gae_lambda': 0.99, 'max_grad_norm': 0.8, 'vf_coef': 0.5550723938974348, 'gamma': 0.999, 'learning_rate': 0.01809, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (42, 50)) of distribution Normal(loc: torch.Size([42, 50]), scale: torch.Size([42, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 32, 'ent_coef': 0.08949030960724885, 'gae_lambda': 0.99, 'max_grad_norm': 0.8, 'vf_coef': 0.5550723938974348, 'gamma': 0.999, 'learning_rate': 0.01809, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (32, 50)) of distribution Normal(loc: torch.Size([32, 50]), scale: torch.Size([32, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 32, 'ent_coef': 0.08949030960724885, 'gae_lambda': 0.99, 'max_grad_norm': 0.8, 'vf_coef': 0.5550723938974348, 'gamma': 0.999, 'learning_rate': 0.01809, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (32, 50)) of distribution Normal(loc: torch.Size([32, 50]), scale: torch.Size([32, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 128, 'ent_coef': 0.022411740255297286, 'gae_lambda': 0.8, 'max_grad_norm': 0.6, 'vf_coef': 0.7377791278704454, 'gamma': 0.98, 'learning_rate': 0.00049, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2018-07-05T00:00:00.000000000 to  2019-01-04T00:00:00.000000000 Model is :  Rec_PPO\n",
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1368366.8983591516\n",
      "end_total_asset:1268078.8787796707\n",
      "total_asset_change:-100288.01957948087\n",
      "Total cash is: 7.839047691270309$ and total holdings in stocks are 1268071.0397319794$\n",
      "Buy & Hold strategy with previous total asset:  1246704.152442993\n",
      "Total Cost:  2565.392766484311\n",
      "Sum of rewards  -105708.3783401458\n",
      "Total trades:  492\n",
      "Total buy orders are 236 and total sell orders are 256\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  -0.09208223173440103\n",
      "=============\n",
      "Total reward for the the window is 38.2852641979116\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1gmc5r22\" target=\"_blank\">iconic-hill-1125</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21116... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1546560000000000000</td></tr><tr><td>end_total_asset</td><td>1268078.87878</td></tr><tr><td>trade_reward</td><td>-0.37301</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">iconic-hill-1125</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1gmc5r22\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/1gmc5r22</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221005_214708-1gmc5r22\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2018-07-05T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.441183376312257  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  3.3066852181218565\n",
      "Total Trades:  3604\n",
      "End total asset for validation 946022.0448098617\n",
      "Mean Reward: -3.436165742389858\n",
      "STD reward: 0.22017717882002288\n",
      "-----------------\n",
      "Reward for the period is 3.3066852181218565\n",
      "Agent #0 has better performance for the training period with total asset: 946022.0448098617\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.97893331448237  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -4.8106853612698615\n",
      "Total Trades:  4957\n",
      "End total asset for validation 800311.4809028604\n",
      "Mean Reward: -1.7723239458660829\n",
      "STD reward: 0.16447905008333594\n",
      "-----------------\n",
      "Reward for the period is -4.8106853612698615\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.504582659403484  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  1.3686831574887037\n",
      "Total Trades:  2691\n",
      "End total asset for validation 897938.1063210629\n",
      "Mean Reward: -3.4259513704600977\n",
      "STD reward: 0.16362849459737158\n",
      "-----------------\n",
      "Reward for the period is 1.3686831574887037\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.477283795674643  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  0.7440673423698172\n",
      "Total Trades:  3884\n",
      "End total asset for validation 896035.3548892791\n",
      "Mean Reward: -3.001987946545705\n",
      "STD reward: 0.280800764211426\n",
      "-----------------\n",
      "Reward for the period is 0.7440673423698172\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.46634999513626  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  0.4008737779222429\n",
      "Total Trades:  3991\n",
      "End total asset for validation 880275.1389518923\n",
      "Mean Reward: -2.9633835949513014\n",
      "STD reward: 0.06678523617076886\n",
      "-----------------\n",
      "Reward for the period is 0.4008737779222429\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.457417317231496  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  0.2658026572316885\n",
      "Total Trades:  3449\n",
      "End total asset for validation 884107.401910036\n",
      "Mean Reward: -2.3045812711890905\n",
      "STD reward: 0.044219705065395506\n",
      "-----------------\n",
      "Reward for the period is 0.2658026572316885\n",
      "{'batch_size': 256, 'ent_coef': 0.06031708978524879, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.6469570281830601, 'gamma': 0.98, 'learning_rate': 0.00532, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.046132063331339436, 'gae_lambda': 0.9, 'max_grad_norm': 0.7, 'vf_coef': 0.8742480074793201, 'gamma': 0.999, 'learning_rate': 0.01231, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.922333840529124  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -7.961215747054666\n",
      "Total Trades:  5124\n",
      "End total asset for validation 823841.0076305016\n",
      "Mean Reward: -2.426614893798251\n",
      "STD reward: 0.08201600710388136\n",
      "-----------------\n",
      "Reward for the period is -7.961215747054666\n",
      "{'batch_size': 32, 'ent_coef': 0.046132063331339436, 'gae_lambda': 0.9, 'max_grad_norm': 0.7, 'vf_coef': 0.8742480074793201, 'gamma': 0.999, 'learning_rate': 0.01231, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.918183326721191  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -7.961215747054666\n",
      "Total Trades:  5124\n",
      "End total asset for validation 823841.0076305016\n",
      "Mean Reward: -3.366934435348958\n",
      "STD reward: 0.003115835414041843\n",
      "-----------------\n",
      "Reward for the period is -7.961215747054666\n",
      "{'batch_size': 32, 'ent_coef': 0.046132063331339436, 'gae_lambda': 0.9, 'max_grad_norm': 0.7, 'vf_coef': 0.8742480074793201, 'gamma': 0.999, 'learning_rate': 0.01231, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.913967216014862  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2018-07-05 00:00:00 to  2019-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -7.961215747054666\n",
      "Total Trades:  5124\n",
      "End total asset for validation 823841.0076305016\n",
      "Mean Reward: -3.851087172626285\n",
      "STD reward: 0.1976352389644441\n",
      "-----------------\n",
      "Reward for the period is -7.961215747054666\n",
      "{'batch_size': 512, 'ent_coef': 0.06339233796928416, 'gae_lambda': 0.8, 'max_grad_norm': 0.7, 'vf_coef': 0.06325227069870609, 'gamma': 0.95, 'learning_rate': 0.02815, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.06339233796928416, 'gae_lambda': 0.8, 'max_grad_norm': 0.7, 'vf_coef': 0.06325227069870609, 'gamma': 0.95, 'learning_rate': 0.02815, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.028872996506254887, 'gae_lambda': 0.9, 'max_grad_norm': 1.0, 'vf_coef': 0.43010066796212054, 'gamma': 0.9, 'learning_rate': 0.04196, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.028872996506254887, 'gae_lambda': 0.9, 'max_grad_norm': 1.0, 'vf_coef': 0.43010066796212054, 'gamma': 0.9, 'learning_rate': 0.04196, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.028872996506254887, 'gae_lambda': 0.9, 'max_grad_norm': 1.0, 'vf_coef': 0.43010066796212054, 'gamma': 0.9, 'learning_rate': 0.04196, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 512, 'ent_coef': 0.09183492562019416, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.12976240431117625, 'gamma': 0.98, 'learning_rate': 0.03352, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (148, 50)) of distribution Normal(loc: torch.Size([148, 50]), scale: torch.Size([148, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2019-01-04T00:00:00.000000000 to  2019-07-08T00:00:00.000000000 Model is :  Rec_PPO\n",
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1268078.8787796707\n",
      "end_total_asset:1625364.426682696\n",
      "total_asset_change:357285.5479030253\n",
      "Total cash is: 8.829322084953631$ and total holdings in stocks are 1625355.597360611$\n",
      "Buy & Hold strategy with previous total asset:  1482660.5643753598\n",
      "Total Cost:  4823.733199604687\n",
      "Sum of rewards  318899.3479308726\n",
      "Total trades:  798\n",
      "Total buy orders are 514 and total sell orders are 284\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  0.5126477256362411\n",
      "=============\n",
      "Total reward for the the window is 62.96750394551782\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1pxfzdly\" target=\"_blank\">visionary-meadow-1126</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16980... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1562544000000000000</td></tr><tr><td>end_total_asset</td><td>1625364.42668</td></tr><tr><td>trade_reward</td><td>-0.15806</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">visionary-meadow-1126</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1pxfzdly\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/1pxfzdly</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221005_233335-1pxfzdly\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2019-01-04T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.477150515715282  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  10.617439522058703\n",
      "Total Trades:  2785\n",
      "End total asset for validation 1086402.6381729594\n",
      "Mean Reward: 2.9173723931569837\n",
      "STD reward: 0.21750318506485158\n",
      "-----------------\n",
      "Reward for the period is 10.617439522058703\n",
      "Agent #0 has better performance for the training period with total asset: 1086402.6381729594\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.953732856114705  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  13.298997785896063\n",
      "Total Trades:  4996\n",
      "End total asset for validation 1144342.0511232372\n",
      "Mean Reward: 6.519314929749816\n",
      "STD reward: 0.1969565556523784\n",
      "-----------------\n",
      "Reward for the period is 13.298997785896063\n",
      "Agent #1 has better performance for the training period with total asset: 1144342.0511232372\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.50368548631668  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  14.39802141673863\n",
      "Total Trades:  3079\n",
      "End total asset for validation 1161074.970544566\n",
      "Mean Reward: 6.223125858942512\n",
      "STD reward: 0.17322573274919442\n",
      "-----------------\n",
      "Reward for the period is 14.39802141673863\n",
      "Agent #2 has better performance for the training period with total asset: 1161074.970544566\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.865333890914917  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  13.480340041453019\n",
      "Total Trades:  3459\n",
      "End total asset for validation 1188394.80189679\n",
      "Mean Reward: 6.647476043691858\n",
      "STD reward: 0.4290496768623697\n",
      "-----------------\n",
      "Reward for the period is 13.480340041453019\n",
      "Agent #3 has better performance for the training period with total asset: 1188394.80189679\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.83421714703242  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  12.877366982866079\n",
      "Total Trades:  3272\n",
      "End total asset for validation 1146455.5619591144\n",
      "Mean Reward: 4.307539711613208\n",
      "STD reward: 0.1126838398021758\n",
      "-----------------\n",
      "Reward for the period is 12.877366982866079\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.735733882586162  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  13.227161673130468\n",
      "Total Trades:  3609\n",
      "End total asset for validation 1168044.159532279\n",
      "Mean Reward: 7.172587037086487\n",
      "STD reward: 0.10577055493117479\n",
      "-----------------\n",
      "Reward for the period is 13.227161673130468\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.439499481519062  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  10.502084366002236\n",
      "Total Trades:  2693\n",
      "End total asset for validation 1096774.508134645\n",
      "Mean Reward: 5.261343415861484\n",
      "STD reward: 0.2856554523989239\n",
      "-----------------\n",
      "Reward for the period is 10.502084366002236\n",
      "{'batch_size': 32, 'ent_coef': 0.007906332085841933, 'gae_lambda': 0.8, 'max_grad_norm': 0.5, 'vf_coef': 0.8889923622870478, 'gamma': 0.99, 'learning_rate': 0.03615, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.037633792559307  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  11.035769478883594\n",
      "Total Trades:  5151\n",
      "End total asset for validation 1099859.0295420487\n",
      "Mean Reward: 6.64776236335747\n",
      "STD reward: 0.3151807270202407\n",
      "-----------------\n",
      "Reward for the period is 11.035769478883594\n",
      "{'batch_size': 128, 'ent_coef': 0.08725738215213545, 'gae_lambda': 0.95, 'max_grad_norm': 1.0, 'vf_coef': 0.35365119921206134, 'gamma': 0.995, 'learning_rate': 0.03357, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 128, 'ent_coef': 0.08725738215213545, 'gae_lambda': 0.95, 'max_grad_norm': 1.0, 'vf_coef': 0.35365119921206134, 'gamma': 0.995, 'learning_rate': 0.03357, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.04948796782919477, 'gae_lambda': 0.8, 'max_grad_norm': 0.8, 'vf_coef': 0.6666367680382049, 'gamma': 0.9999, 'learning_rate': 0.01886, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.04948796782919477, 'gae_lambda': 0.8, 'max_grad_norm': 0.8, 'vf_coef': 0.6666367680382049, 'gamma': 0.9999, 'learning_rate': 0.01886, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.04948796782919477, 'gae_lambda': 0.8, 'max_grad_norm': 0.8, 'vf_coef': 0.6666367680382049, 'gamma': 0.9999, 'learning_rate': 0.01886, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.08620927113950247, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.21291474043605962, 'gamma': 0.999, 'learning_rate': 0.00179, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time  jimmy_reccurent_ppo-v1 :  9.674183817704519  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  11.498931767244358\n",
      "Total Trades:  5187\n",
      "End total asset for validation 1101564.5553527582\n",
      "Mean Reward: 6.491802032117266\n",
      "STD reward: 0.18869927477539417\n",
      "-----------------\n",
      "Reward for the period is 11.498931767244358\n",
      "{'batch_size': 64, 'ent_coef': 0.08620927113950247, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.21291474043605962, 'gamma': 0.999, 'learning_rate': 0.00179, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  9.647233295440675  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  12.295317726093344\n",
      "Total Trades:  5077\n",
      "End total asset for validation 1125908.65033071\n",
      "Mean Reward: 6.13210961947334\n",
      "STD reward: 0.3086292793146059\n",
      "-----------------\n",
      "Reward for the period is 12.295317726093344\n",
      "{'batch_size': 64, 'ent_coef': 0.08620927113950247, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.21291474043605962, 'gamma': 0.999, 'learning_rate': 0.00179, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  9.66416665315628  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  9.768619351787493\n",
      "Total Trades:  5152\n",
      "End total asset for validation 1089014.596107061\n",
      "Mean Reward: 5.664789171190932\n",
      "STD reward: 0.8733175204502006\n",
      "-----------------\n",
      "Reward for the period is 9.768619351787493\n",
      "{'batch_size': 32, 'ent_coef': 0.000149632241102532, 'gae_lambda': 0.92, 'max_grad_norm': 1.0, 'vf_coef': 0.2927621499368016, 'gamma': 0.98, 'learning_rate': 0.01507, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.845216751098633  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-01-04 00:00:00 to  2019-07-05 00:00:00\n",
      "-----------------\n",
      "Total Reward:  14.732820512726903\n",
      "Total Trades:  3105\n",
      "End total asset for validation 1158329.3850984166\n",
      "Mean Reward: 7.551040634792298\n",
      "STD reward: 0.08456368590798315\n",
      "-----------------\n",
      "Reward for the period is 14.732820512726903\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2019-07-08T00:00:00.000000000 to  2020-01-06T00:00:00.000000000 Model is :  Rec_PPO\n",
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1625364.426682696\n",
      "end_total_asset:1721168.3781145809\n",
      "total_asset_change:95803.9514318849\n",
      "Total cash is: 14.554918169779114$ and total holdings in stocks are 1721153.8231964111$\n",
      "Stock was not active BRK\n",
      "Buy & Hold strategy with previous total asset:  1794014.3045822098\n",
      "Total Cost:  3068.9906476593023\n",
      "Sum of rewards  104323.74835075391\n",
      "Total trades:  484\n",
      "Total buy orders are 231 and total sell orders are 253\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  0.11138391255158384\n",
      "=============\n",
      "Total reward for the the window is 67.50109285901999\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1wuit1yo\" target=\"_blank\">grateful-dream-1127</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10624... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1578268800000000000</td></tr><tr><td>end_total_asset</td><td>1721168.37811</td></tr><tr><td>trade_reward</td><td>-0.36393</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">grateful-dream-1127</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/1wuit1yo\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/1wuit1yo</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221006_014632-1wuit1yo\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2019-07-08T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.153349991639455  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  3.5776643837307347\n",
      "Total Trades:  3084\n",
      "End total asset for validation 1144249.817723965\n",
      "Mean Reward: 1.8022172576876982\n",
      "STD reward: 0.23508243341873425\n",
      "-----------------\n",
      "Reward for the period is 3.5776643837307347\n",
      "Agent #0 has better performance for the training period with total asset: 1144249.817723965\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.836499961217244  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -0.42203616444021463\n",
      "Total Trades:  4816\n",
      "End total asset for validation 1052689.4894153567\n",
      "Mean Reward: 1.9566402748227119\n",
      "STD reward: 0.06927210312570552\n",
      "-----------------\n",
      "Reward for the period is -0.42203616444021463\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.194983398914337  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  2.662405118579045\n",
      "Total Trades:  3497\n",
      "End total asset for validation 1009567.877577119\n",
      "Mean Reward: 1.375016052450519\n",
      "STD reward: 0.2594364821821537\n",
      "-----------------\n",
      "Reward for the period is 2.662405118579045\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.309733323256175  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  5.987793253356358\n",
      "Total Trades:  2975\n",
      "End total asset for validation 1117592.9193653637\n",
      "Mean Reward: 2.2835539541731125\n",
      "STD reward: 0.10556789761244792\n",
      "-----------------\n",
      "Reward for the period is 5.987793253356358\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.229333321253458  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  1.4626378552784445\n",
      "Total Trades:  3048\n",
      "End total asset for validation 1077334.9179553029\n",
      "Mean Reward: 1.0994691418229194\n",
      "STD reward: 0.01127682978318787\n",
      "-----------------\n",
      "Reward for the period is 1.4626378552784445\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.32495038509369  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  0.06094227638095617\n",
      "Total Trades:  3285\n",
      "End total asset for validation 1054790.2045521406\n",
      "Mean Reward: 1.3237987422035076\n",
      "STD reward: 0.05655014584220865\n",
      "-----------------\n",
      "Reward for the period is 0.06094227638095617\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.193283768494924  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  1.2653068057261407\n",
      "Total Trades:  2379\n",
      "End total asset for validation 1056435.8358466742\n",
      "Mean Reward: 0.9583779754815623\n",
      "STD reward: 0.007945673286441066\n",
      "-----------------\n",
      "Reward for the period is 1.2653068057261407\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.238750485579173  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  2.3448842936195433\n",
      "Total Trades:  2950\n",
      "End total asset for validation 1050333.7157584494\n",
      "Mean Reward: 2.3040950321126727\n",
      "STD reward: 0.08363247133888013\n",
      "-----------------\n",
      "Reward for the period is 2.3448842936195433\n",
      "{'batch_size': 512, 'ent_coef': 0.06295701629083916, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'vf_coef': 0.6361541839020944, 'gamma': 0.995, 'learning_rate': 0.03162, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  9.963417275746663  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -0.9336072726873681\n",
      "Total Trades:  4977\n",
      "End total asset for validation 1032514.4737576311\n",
      "Mean Reward: 2.4184372829273344\n",
      "STD reward: 0.2668640537969631\n",
      "-----------------\n",
      "Reward for the period is -0.9336072726873681\n",
      "{'batch_size': 128, 'ent_coef': 0.08604605175133549, 'gae_lambda': 0.8, 'max_grad_norm': 0.9, 'vf_coef': 0.5928362737467932, 'gamma': 0.995, 'learning_rate': 0.04558, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.0317783184139186, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.3076068554106288, 'gamma': 0.9999, 'learning_rate': 0.03776, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.0317783184139186, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.3076068554106288, 'gamma': 0.9999, 'learning_rate': 0.03776, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.0317783184139186, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.3076068554106288, 'gamma': 0.9999, 'learning_rate': 0.03776, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.05407606788494298, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.3841585396863446, 'gamma': 0.999, 'learning_rate': 0.00591, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time  jimmy_reccurent_ppo-v1 :  11.669483816623687  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -0.9336072726873681\n",
      "Total Trades:  4977\n",
      "End total asset for validation 1032514.4737576311\n",
      "Mean Reward: 3.1009679287672043\n",
      "STD reward: 0.4054271369017695\n",
      "-----------------\n",
      "Reward for the period is -0.9336072726873681\n",
      "{'batch_size': 32, 'ent_coef': 0.05407606788494298, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.3841585396863446, 'gamma': 0.999, 'learning_rate': 0.00591, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.671817111968995  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2019-07-08 00:00:00 to  2020-01-03 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -0.9336072726873681\n",
      "Total Trades:  4977\n",
      "End total asset for validation 1032514.4737576311\n",
      "Mean Reward: 1.5427406648057513\n",
      "STD reward: 0.05425506784112577\n",
      "-----------------\n",
      "Reward for the period is -0.9336072726873681\n",
      "{'batch_size': 64, 'ent_coef': 0.05845737994360871, 'gae_lambda': 0.9, 'max_grad_norm': 0.9, 'vf_coef': 0.1352934402450322, 'gamma': 0.99, 'learning_rate': 0.04299, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.05845737994360871, 'gae_lambda': 0.9, 'max_grad_norm': 0.9, 'vf_coef': 0.1352934402450322, 'gamma': 0.99, 'learning_rate': 0.04299, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.05845737994360871, 'gae_lambda': 0.9, 'max_grad_norm': 0.9, 'vf_coef': 0.1352934402450322, 'gamma': 0.99, 'learning_rate': 0.04299, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2020-01-06T00:00:00.000000000 to  2020-07-07T00:00:00.000000000 Model is :  Rec_PPO\n",
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1721168.3781145809\n",
      "end_total_asset:1410461.249913654\n",
      "total_asset_change:-310707.1282009268\n",
      "Total cash is: 20.766165931783384$ and total holdings in stocks are 1410440.4837477223$\n",
      "Stock was not active BRK\n",
      "Buy & Hold strategy with previous total asset:  1607696.7860683268\n",
      "Total Cost:  15089.771850882198\n",
      "Sum of rewards  -315010.03274575714\n",
      "Total trades:  1911\n",
      "Total buy orders are 1005 and total sell orders are 906\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  -0.06298333742854514\n",
      "=============\n",
      "Total reward for the the window is 51.89123763382668\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/16f8e8gr\" target=\"_blank\">dandy-wood-1128</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17860... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1594080000000000000</td></tr><tr><td>end_total_asset</td><td>1410461.24991</td></tr><tr><td>trade_reward</td><td>0.14003</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dandy-wood-1128</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/16f8e8gr\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/16f8e8gr</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221006_035829-16f8e8gr\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2020-01-06T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.286200523376465  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -3.9819639660418034\n",
      "Total Trades:  3742\n",
      "End total asset for validation 958234.1720387505\n",
      "Mean Reward: -0.254363774927333\n",
      "STD reward: 0.13284832050937745\n",
      "-----------------\n",
      "Reward for the period is -3.9819639660418034\n",
      "Agent #0 has better performance for the training period with total asset: 958234.1720387505\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.8324995358785  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -7.938583617564291\n",
      "Total Trades:  4760\n",
      "End total asset for validation 854755.8184629319\n",
      "Mean Reward: -0.46978124999441206\n",
      "STD reward: 0.0491091813728578\n",
      "-----------------\n",
      "Reward for the period is -7.938583617564291\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.401250485579173  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -3.362273409264162\n",
      "Total Trades:  3397\n",
      "End total asset for validation 946855.7002815953\n",
      "Mean Reward: -0.4415414328686893\n",
      "STD reward: 0.0452355278311737\n",
      "-----------------\n",
      "Reward for the period is -3.362273409264162\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.21810003121694  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -8.268419867381454\n",
      "Total Trades:  2553\n",
      "End total asset for validation 824335.8070993661\n",
      "Mean Reward: -1.1361432745936326\n",
      "STD reward: 0.09200978051004616\n",
      "-----------------\n",
      "Reward for the period is -8.268419867381454\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.328833361466726  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -4.307232995517552\n",
      "Total Trades:  3649\n",
      "End total asset for validation 860722.194531464\n",
      "Mean Reward: -0.7299565439345315\n",
      "STD reward: 0.3025186273264018\n",
      "-----------------\n",
      "Reward for the period is -4.307232995517552\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.300483842690786  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -7.376726896327455\n",
      "Total Trades:  3613\n",
      "End total asset for validation 817060.2736714921\n",
      "Mean Reward: -0.670811000012327\n",
      "STD reward: 0.04281614638146536\n",
      "-----------------\n",
      "Reward for the period is -7.376726896327455\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.32118288675944  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -0.637065639719367\n",
      "Total Trades:  3590\n",
      "End total asset for validation 1010467.3358568875\n",
      "Mean Reward: -0.06990420253714547\n",
      "STD reward: 0.08923897554304787\n",
      "-----------------\n",
      "Reward for the period is -0.637065639719367\n",
      "Agent #6 has better performance for the training period with total asset: 1010467.3358568875\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.315883322556813  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -6.156121374573559\n",
      "Total Trades:  3854\n",
      "End total asset for validation 913738.7475210831\n",
      "Mean Reward: -0.06728028383513447\n",
      "STD reward: 0.2288006841041499\n",
      "-----------------\n",
      "Reward for the period is -6.156121374573559\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.247300493717194  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -3.124611967243254\n",
      "Total Trades:  3527\n",
      "End total asset for validation 908121.7497781736\n",
      "Mean Reward: -0.4385945194400847\n",
      "STD reward: 0.08071004089352098\n",
      "-----------------\n",
      "Reward for the period is -3.124611967243254\n",
      "{'batch_size': 32, 'ent_coef': 0.0614604067675787, 'gae_lambda': 1.0, 'max_grad_norm': 0.9, 'vf_coef': 0.400403005274946, 'gamma': 0.9999, 'learning_rate': 0.01046, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  11.91440003712972  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -3.0405449941463303\n",
      "Total Trades:  4964\n",
      "End total asset for validation 871811.6463035373\n",
      "Mean Reward: -0.6294565907679498\n",
      "STD reward: 0.044559765545882514\n",
      "-----------------\n",
      "Reward for the period is -3.0405449941463303\n",
      "{'batch_size': 32, 'ent_coef': 0.0614604067675787, 'gae_lambda': 1.0, 'max_grad_norm': 0.9, 'vf_coef': 0.400403005274946, 'gamma': 0.9999, 'learning_rate': 0.01046, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time  jimmy_reccurent_ppo-v1 :  11.883566570281982  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-01-06 00:00:00 to  2020-07-06 00:00:00\n",
      "-----------------\n",
      "Total Reward:  -3.0405449941463303\n",
      "Total Trades:  4964\n",
      "End total asset for validation 871811.6463035373\n",
      "Mean Reward: -1.119330525281839\n",
      "STD reward: 0.04497723332463514\n",
      "-----------------\n",
      "Reward for the period is -3.0405449941463303\n",
      "{'batch_size': 64, 'ent_coef': 0.09734441635468326, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'vf_coef': 0.037058848982533865, 'gamma': 0.995, 'learning_rate': 0.04612, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (64, 50)) of distribution Normal(loc: torch.Size([64, 50]), scale: torch.Size([64, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 64, 'ent_coef': 0.09734441635468326, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'vf_coef': 0.037058848982533865, 'gamma': 0.995, 'learning_rate': 0.04612, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (64, 50)) of distribution Normal(loc: torch.Size([64, 50]), scale: torch.Size([64, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 512, 'ent_coef': 0.055399922037404056, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.6701938231677522, 'gamma': 0.95, 'learning_rate': 0.04743, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (128, 50)) of distribution Normal(loc: torch.Size([128, 50]), scale: torch.Size([128, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 512, 'ent_coef': 0.055399922037404056, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.6701938231677522, 'gamma': 0.95, 'learning_rate': 0.04743, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (152, 50)) of distribution Normal(loc: torch.Size([152, 50]), scale: torch.Size([152, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 512, 'ent_coef': 0.055399922037404056, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.6701938231677522, 'gamma': 0.95, 'learning_rate': 0.04743, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Hardswish'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "Expected parameter loc (Tensor of shape (148, 50)) of distribution Normal(loc: torch.Size([148, 50]), scale: torch.Size([148, 50])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'batch_size': 32, 'ent_coef': 0.05484346764678014, 'gae_lambda': 0.98, 'max_grad_norm': 0.6, 'vf_coef': 0.7860266367955061, 'gamma': 0.999, 'learning_rate': 0.00632, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 32, 'ent_coef': 0.05484346764678014, 'gae_lambda': 0.98, 'max_grad_norm': 0.6, 'vf_coef': 0.7860266367955061, 'gamma': 0.999, 'learning_rate': 0.00632, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "{'batch_size': 64, 'ent_coef': 0.06591496869643299, 'gae_lambda': 0.8, 'max_grad_norm': 0.3, 'vf_coef': 0.4753549660344746, 'gamma': 0.95, 'learning_rate': 0.03715, 'clip_range': 0.4, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'shared_lstm': True}, 'seed': 66}\n",
      "Model destabilized with params:   Creating new params\n",
      "You must choose between shared LSTM, seperate or no LSTM for the critic\n",
      "Sharpe Ratio:  0\n",
      "================================================================================\n",
      "Best params,  {'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "================================================================================\n",
      "======Trading from:  2020-07-07T00:00:00.000000000 to  2021-01-05T00:00:00.000000000 Model is :  Rec_PPO\n",
      "Saving to  results/account_value_trade_main_jimmy_reccurent_ppo-v1.csv\n",
      "previous_total_asset:1410461.249913654\n",
      "end_total_asset:1577621.5243607222\n",
      "total_asset_change:167160.2744470681\n",
      "Total cash is: 19.682143123898346$ and total holdings in stocks are 1577601.8422175983$\n",
      "Stock was not active BRK\n",
      "Buy & Hold strategy with previous total asset:  1687827.0434848918\n",
      "Total Cost:  1405.1128193429524\n",
      "Sum of rewards  190257.7651893173\n",
      "Total trades:  241\n",
      "Total buy orders are 127 and total sell orders are 114\n",
      "Total days in turbulance:  0\n",
      "Sharpe:  0.13919045190428053\n",
      "=============\n",
      "Total reward for the the window is 53.632975598506164\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.13.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kaanb/FundSimulator/runs/2qyq7izp\" target=\"_blank\">dazzling-snow-1129</a></strong> to <a href=\"https://wandb.ai/kaanb/FundSimulator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2984... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>▁</td></tr><tr><td>end_total_asset</td><td>▁</td></tr><tr><td>trade_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date</td><td>1609804800000000000</td></tr><tr><td>end_total_asset</td><td>1577621.52436</td></tr><tr><td>trade_reward</td><td>-0.28136</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dazzling-snow-1129</strong>: <a href=\"https://wandb.ai/kaanb/FundSimulator/runs/2qyq7izp\" target=\"_blank\">https://wandb.ai/kaanb/FundSimulator/runs/2qyq7izp</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20221006_063127-2qyq7izp\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "======Model training from:  2011-01-01 to  2020-07-07T00:00:00.000000000\n",
      "======Training Agents with the population of 10========\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.377450466156006  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  5.722559251473285\n",
      "Total Trades:  3271\n",
      "End total asset for validation 1148708.8555316483\n",
      "Mean Reward: 2.396920195611892\n",
      "STD reward: 0.32857539855802065\n",
      "-----------------\n",
      "Reward for the period is 5.722559251473285\n",
      "Agent #0 has better performance for the training period with total asset: 1148708.8555316483\n",
      "{'batch_size': 32, 'ent_coef': 0.0165006032313881, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.8714603844285117, 'gamma': 0.99, 'learning_rate': 0.04536, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [512, 128, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 1, 'shared_lstm': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.002716632684072  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  1.5140539247076958\n",
      "Total Trades:  4830\n",
      "End total asset for validation 1072694.9368988804\n",
      "Mean Reward: 3.4234991511504633\n",
      "STD reward: 0.0558734681854383\n",
      "-----------------\n",
      "Reward for the period is 1.5140539247076958\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.346999951203664  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  12.593874062993564\n",
      "Total Trades:  3580\n",
      "End total asset for validation 1209229.4350919912\n",
      "Mean Reward: 3.7225409543316346\n",
      "STD reward: 0.2691255145340268\n",
      "-----------------\n",
      "Reward for the period is 12.593874062993564\n",
      "Agent #2 has better performance for the training period with total asset: 1209229.4350919912\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.418066724141438  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  10.480666532064788\n",
      "Total Trades:  3444\n",
      "End total asset for validation 1159715.7018163053\n",
      "Mean Reward: 3.151038890791824\n",
      "STD reward: 0.1891471270846105\n",
      "-----------------\n",
      "Reward for the period is 10.480666532064788\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.40556606054306  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  13.94182181643555\n",
      "Total Trades:  3524\n",
      "End total asset for validation 1259992.0140953746\n",
      "Mean Reward: 2.771370924868097\n",
      "STD reward: 0.22219243099139374\n",
      "-----------------\n",
      "Reward for the period is 13.94182181643555\n",
      "Agent #4 has better performance for the training period with total asset: 1259992.0140953746\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.445733416080476  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  10.663096656091511\n",
      "Total Trades:  3253\n",
      "End total asset for validation 1174458.9846716076\n",
      "Mean Reward: 2.5509850680129604\n",
      "STD reward: 0.2994074042262619\n",
      "-----------------\n",
      "Reward for the period is 10.663096656091511\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.504849513371786  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  4.3091147863306105\n",
      "Total Trades:  3629\n",
      "End total asset for validation 1107422.8237842387\n",
      "Mean Reward: 2.47246138395858\n",
      "STD reward: 0.5189700810323685\n",
      "-----------------\n",
      "Reward for the period is 4.3091147863306105\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.501216665903728  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  12.458974811248481\n",
      "Total Trades:  3581\n",
      "End total asset for validation 1191116.5274150483\n",
      "Mean Reward: 3.707934620980086\n",
      "STD reward: 0.505734631577124\n",
      "-----------------\n",
      "Reward for the period is 12.458974811248481\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.396933766206105  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  11.141404651338235\n",
      "Total Trades:  3007\n",
      "End total asset for validation 1186727.7676306174\n",
      "Mean Reward: 3.072510354290716\n",
      "STD reward: 0.7449620463116308\n",
      "-----------------\n",
      "Reward for the period is 11.141404651338235\n",
      "{'n_steps': 8, 'batch_size': 8, 'gamma': 0.95, 'learning_rate': 0.027651081986547765, 'ent_coef': 0.009004220378275581, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.9311987337638572, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'ortho_init': False}, 'seed': 42069}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  12.369166668256124  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Total Reward:  12.35814853780903\n",
      "Total Trades:  3159\n",
      "End total asset for validation 1156534.605535051\n",
      "Mean Reward: 3.3837635717354715\n",
      "STD reward: 0.22685002741384594\n",
      "-----------------\n",
      "Reward for the period is 12.35814853780903\n",
      "{'batch_size': 128, 'ent_coef': 0.09012089395864403, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'vf_coef': 0.12061656519500952, 'gamma': 0.995, 'learning_rate': 0.0118, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.135783243179322  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  8.230115237645805\n",
      "Total Trades:  4994\n",
      "End total asset for validation 1103057.786502553\n",
      "Mean Reward: 3.6669952860102057\n",
      "STD reward: 0.3433416302374492\n",
      "-----------------\n",
      "Reward for the period is 8.230115237645805\n",
      "{'batch_size': 128, 'ent_coef': 0.09012089395864403, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'vf_coef': 0.12061656519500952, 'gamma': 0.995, 'learning_rate': 0.0118, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.118633278210957  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  8.230115237645805\n",
      "Total Trades:  4994\n",
      "End total asset for validation 1103057.786502553\n",
      "Mean Reward: 3.6453677204903214\n",
      "STD reward: 0.19559834192707679\n",
      "-----------------\n",
      "Reward for the period is 8.230115237645805\n",
      "{'batch_size': 128, 'ent_coef': 0.09012089395864403, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'vf_coef': 0.12061656519500952, 'gamma': 0.995, 'learning_rate': 0.0118, 'clip_range': 0.2, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.165533828735352  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  8.230115237645805\n",
      "Total Trades:  4994\n",
      "End total asset for validation 1103057.786502553\n",
      "Mean Reward: 2.8038407508516685\n",
      "STD reward: 0.12682556093448286\n",
      "-----------------\n",
      "Reward for the period is 8.230115237645805\n",
      "{'batch_size': 256, 'ent_coef': 0.041446900614554, 'gae_lambda': 0.92, 'max_grad_norm': 0.7, 'vf_coef': 0.21362074081687876, 'gamma': 0.995, 'learning_rate': 0.0087, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  9.882716643810273  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  5.442226354032755\n",
      "Total Trades:  4840\n",
      "End total asset for validation 1091167.2968146224\n",
      "Mean Reward: 3.35737088174792\n",
      "STD reward: 0.22274102408088528\n",
      "-----------------\n",
      "Reward for the period is 5.442226354032755\n",
      "{'batch_size': 256, 'ent_coef': 0.041446900614554, 'gae_lambda': 0.92, 'max_grad_norm': 0.7, 'vf_coef': 0.21362074081687876, 'gamma': 0.995, 'learning_rate': 0.0087, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  9.968533798058827  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  9.40821467898786\n",
      "Total Trades:  4980\n",
      "End total asset for validation 1136145.5661442522\n",
      "Mean Reward: 1.855802391609177\n",
      "STD reward: 0.10646692425552474\n",
      "-----------------\n",
      "Reward for the period is 9.40821467898786\n",
      "{'batch_size': 256, 'ent_coef': 0.041446900614554, 'gae_lambda': 0.92, 'max_grad_norm': 0.7, 'vf_coef': 0.21362074081687876, 'gamma': 0.995, 'learning_rate': 0.0087, 'clip_range': 0.3, 'policy_kwargs': {'net_arch': [{'pi': [256, 128, 64], 'vf': [64]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'lstm_hidden_size': 256, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n",
      "Training time  jimmy_reccurent_ppo-v1 :  10.04175591468811  minutes\n",
      "======jimmy_reccurent_ppo-v1 Validation from:  2020-07-07 00:00:00 to  2021-01-04 00:00:00\n",
      "-----------------\n",
      "Total Reward:  9.895713938167319\n",
      "Total Trades:  4930\n",
      "End total asset for validation 1115425.6892280313\n",
      "Mean Reward: 2.5303943761158734\n",
      "STD reward: 0.050720567732990045\n",
      "-----------------\n",
      "Reward for the period is 9.895713938167319\n",
      "{'batch_size': 128, 'ent_coef': 0.05466175784303575, 'gae_lambda': 1.0, 'max_grad_norm': 0.3, 'vf_coef': 0.27657939562659584, 'gamma': 0.95, 'learning_rate': 0.02707, 'clip_range': 0.1, 'policy_kwargs': {'net_arch': [{'pi': [256, 64], 'vf': [256, 128]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'lstm_hidden_size': 128, 'n_lstm_layers': 2, 'shared_lstm': False}, 'seed': 66}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataset='datasets/dow50.csv', timesteps=45550, \n",
    "              load=False, model_to_load='model_to_load', normalize =NORNAMLIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretrain_set = pd.read_csv('datasets/pretrain_set3.csv')\n",
    "pretrain_set['turbulence'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = trainer.pretrain(sn, '2011-05-01', START_TRADE, NORNAMLIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = pretrain.split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.cluster(False, period=365, number_of_clusters=5, stocks_per_cluster=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv('./datasets/.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1284/2724465645.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
